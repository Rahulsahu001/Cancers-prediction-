

Breast cancer Predictions 


I have used the Wisconsin Breast cancer dataset from the UCI Machine Learning Repository .
I have used Artificial Neural Networks for this problem and found out the best hyper parameters using cross validation.

Data Set Information:

Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found on website 

Attribute Information:

ID number

Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

a) radius (mean of distances from center to points on the perimeter)
b) texture (standard deviation of gray-scale values)
c) perimeter
d) area
e) smoothness (local variation in radius lengths)
f) compactness (���������2����−1.0)
g) concavity (severity of concave portions of the contour)
h) concave points (number of concave portions of the contour)
i) symmetry
j) fractal dimension ("coastline approximation" - 1)

Import required libraries

We use matplotlib and seaborn to create some wonderful visualizations of our data.
pandas to read our data and know some insights about the data, efficiently. With pandas by our side we can do many things with just simple functions which we will take a look at in the later part.
sklearn to

Select the model with best hyper parameters

Encode the labels i.e. M and B

Print a confusion matrix with test data results

Make a train / test split easily

Scale values

tensorflow and keras to create our model (ANN) and make some plots of it

In [0]:

import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import pandas as pd from keras.wrappers.scikit_learn import KerasClassifier from sklearn.model_selection import GridSearchCV from keras.models import Sequential from keras.layers import Dense from keras.utils.vis_utils import plot_model from IPython.display import SVG from keras.utils import model_to_dot from sklearn.metrics import confusion_matrix from sklearn.preprocessing import LabelEncoder import seaborn as sns dataset = pd.read_csv('wdbc.data') 

Analyze the dataset

We will now analyze our data :

1.Print the features of dataset which are also mentioned above
2.View how many samples and missing values there are for each feature and display them accordingly
We here see the missing samples and values:

RangeIndex: 568 entries, 0 to 567 Data columns (total 32 columns): 842302 568 non-null int64 M 568 non-null object 17.99 568 non-null float64 10.38 568 non-null float64 122.8 568 non-null float64 1001 568 non-null float64 0.1184 568 non-null float64 0.2776 568 non-null float64 ... ... 0.4601 568 non-null float64 0.1189 568 non-null float64 dtypes: float64(30), int64(1), object(1) memory usage: 142.1+ KB None 

3.View numerical features of data set we will majorly focus on mean, count, std, min value, max value, upper quartile, inter quartile and lower quartile
For this all we need to do is dataset.describe() So this justifies our use of pandas library.
4.We will now take a look at the label features specially count, unique, top and frequency. The count parameter just tells us the number of entries, the unique parameter is important. Here we receive -

unique: 2 

Which tells us to perform2 class classification.
The top parameter is often used to check biases in the data set itself.

In [0]:

def analyze(data): # View features in data set print("Dataset Features") print(data.columns.values) print("=" * 30) # View How many samples and how many missing values for each feature print("Dataset Features Details") print(data.info()) print("=" * 30) # view distribution of numerical features across the data set print("Dataset Numerical Features") print(data.describe()) print("=" * 30) # view distribution of categorical features across the data set print("Dataset Categorical Features") print(data.describe(include=['O'])) print("=" * 30) 

In [0]:

analyze(dataset) 

Dataset Features ['842302' 'M' '17.99' '10.38' '122.8' '1001' '0.1184' '0.2776' '0.3001' '0.1471' '0.2419' '0.07871' '1.095' '0.9053' '8.589' '153.4' '0.006399' '0.04904' '0.05373' '0.01587' '0.03003' '0.006193' '25.38' '17.33' '184.6' '2019' '0.1622' '0.6656' '0.7119' '0.2654' '0.4601' '0.1189'] ============================== Dataset Features Details <class 'pandas.core.frame.DataFrame'> RangeIndex: 568 entries, 0 to 567 Data columns (total 32 columns): 842302 568 non-null int64 M 568 non-null object 17.99 568 non-null float64 10.38 568 non-null float64 122.8 568 non-null float64 1001 568 non-null float64 0.1184 568 non-null float64 0.2776 568 non-null float64 0.3001 568 non-null float64 0.1471 568 non-null float64 0.2419 568 non-null float64 0.07871 568 non-null float64 1.095 568 non-null float64 0.9053 568 non-null float64 8.589 568 non-null float64 153.4 568 non-null float64 0.006399 568 non-null float64 0.04904 568 non-null float64 0.05373 568 non-null float64 0.01587 568 non-null float64 0.03003 568 non-null float64 0.006193 568 non-null float64 25.38 568 non-null float64 17.33 568 non-null float64 184.6 568 non-null float64 2019 568 non-null float64 0.1622 568 non-null float64 0.6656 568 non-null float64 0.7119 568 non-null float64 0.2654 568 non-null float64 0.4601 568 non-null float64 0.1189 568 non-null float64 dtypes: float64(30), int64(1), object(1) memory usage: 142.1+ KB None ============================== Dataset Numerical Features 842302 17.99 10.38 ... 0.2654 0.4601 0.1189 count 5.680000e+02 568.000000 568.000000 ... 568.000000 568.000000 568.000000 mean 3.042382e+07 14.120491 19.305335 ... 0.114341 0.289776 0.083884 std 1.251246e+08 3.523416 4.288506 ... 0.065484 0.061508 0.018017 min 8.670000e+03 6.981000 9.710000 ... 0.000000 0.156500 0.055040 25% 8.692225e+05 11.697500 16.177500 ... 0.064730 0.250350 0.071412 50% 9.061570e+05 13.355000 18.855000 ... 0.099840 0.282050 0.080015 75% 8.825022e+06 15.780000 21.802500 ... 0.161325 0.317675 0.092065 max 9.113205e+08 28.110000 39.280000 ... 0.291000 0.663800 0.207500 [8 rows x 31 columns] ============================== Dataset Categorical Features M count 568 unique 2 top B freq 357 ============================== 

Make a feature pairplot

We will now make a feature wise pairplot meaning we will plot labels �1, �2, ... and or label � with each other. Where � and � have their usual meaning. We will use seaborn to help us with this. A pair plot allows us to see both distribution of single variables and relationships between two variables . Pair plots are a great method to identify trends for follow-up analysis. So this again becomes an important step for us.

In [0]:

sns.pairplot(dataset, hue="M", size= 2.5) 

/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:2065: UserWarning: The `size` parameter has been renamed to `height`; pleaes update your code. warnings.warn(msg, UserWarning) 

Out[0]:

<seaborn.axisgrid.PairGrid at 0x7f5ecb908e80>

￼

Seperate the the features and laabels

This is just a simple code which stores � or the features and � or the labels in different variables

In [0]:

X = dataset.iloc[:,2:32] y = dataset.iloc[:,1] 

Encode the labels to 1, 0

We had our labels as M and B depicting malignant and benign respectively. As these are strings or char as you might say and we are only concerned with numbers so we encode such that

All M = 1

All B = 0

For this we use the LabelEncoder class.

In [0]:

print("Earlier: ") print(y[100:110]) labelencoder_Y = LabelEncoder() y = labelencoder_Y.fit_transform(y) print() print("After: ") print(y[100:110]) 

Earlier: 100 B 101 B 102 B 103 B 104 M 105 B 106 B 107 M 108 B 109 B Name: M, dtype: object After: [0 0 0 0 1 0 0 1 0 0] 

Make a 80/ 20 train, test split

Making a train test split is important for any AI problem without which we do not know how our model would perform to unseen values and also not overfit the data

In [0]:

from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) 

Scaling the values is not actually compulsory but I would recommend one to do it for a faster convergence, so we use sklearn to help us in this

In [0]:

# Scale values from faster convergence from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) 

Build a classifier

We finally build a tensorflow, keras classifier

In [0]:

def build_classifier(optimizer): classifier = Sequential() classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30)) classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu')) classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy']) return classifier 

In [0]:

classifier = KerasClassifier(build_fn = build_classifier) 

Experiment with various hyper parameters

Before tuning our hyper parameters we surely want to test the data with various of them and choose the best one so define few options for the model

In [0]:

parameters = {'batch_size': [1, 5], 'epochs': [100, 120], 'optimizer': ['adam', 'rmsprop']} 

Use Cross Vaildation to obtain best model

Cross Vaalidation is a wonderful technique which often comes to our rescue while selecting the best model so we use a 10 fold CV here.
We could also have used AIC, BIC or even Mallows �� if the CV does not give us a good result but that's not the case here

In [0]:

# Cross validation grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 10) 

Search for the best model in the complete matrix

In [0]:

# Get best model # Note: this may take some time grid_search = grid_search.fit(X_train, y_train) 

Finally build model according to above obtained results

In the above step we already obtained the best model for this problem so now we are almost done all we need to do is build a classifier according to obtained results

In [0]:

classifier = Sequential() 

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. 

So we make an ANN now with

Input Layer - 16 neurons and ReLu activator

Hidden Layer 1 - 8 neurons and ReLu activator

Hidden Layer 2 - 6 neurons and ReLu activator

Output Layer - 1 neuron (ok that was obvious !) and sigmoid activator

In [0]:

# Make the best classifier as we received earlier classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30)) classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu')) classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) 

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. 

Complete the classifier

We use BCE or binary cross entropy which is suited best to sigmoid.
���=��1+��2
���=−�����^−(1−�)���(1−�^)

In [0]:

classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy']) 

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where 

Fit the classifier to the data

Finally fit the classifier to training data

In [0]:

classifier.fit(X_train, y_train, batch_size = 1, epochs = 100, verbose=1) 

Epoch 1/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 2/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 3/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 4/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 5/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 6/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 7/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 8/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 9/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 10/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 11/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 12/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 13/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 14/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 15/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 16/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 17/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 18/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 19/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 20/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 21/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 22/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 23/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 24/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 25/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 26/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 27/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 28/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 29/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 30/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 31/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 32/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 33/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 34/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 35/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 36/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 37/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 38/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 39/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 40/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 41/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 42/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 43/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 44/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 45/100 454/454 [==============================] - 0s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 46/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 47/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 48/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 49/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 50/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 51/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 52/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 53/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 54/100 454/454 [==============================] - 0s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 55/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 56/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 57/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 58/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 59/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 60/100 454/454 [==============================] - 0s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 61/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 62/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 63/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 64/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 65/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 66/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 67/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 68/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 69/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 70/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 71/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 72/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 73/100 454/454 [==============================] - 1s 1ms/step - loss: 1.0711e-07 - acc: 1.0000 Epoch 74/100 454/454 [===============
